# Step2: Direct Preference Optimization: Your Language Model is Secretly a Reward Model

paper: https://arxiv.org/abs/2305.18290

## How to train the model

```bash
bash examples/DPO/train.sh
```

## Citation

```
@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}
```
