cmd = deepspeed main.py --data_path Dahoas/rm-static Dahoas/full-hh-rlhf Dahoas/synthetic-instruct-gptj-pairwise yitingxie/rlhf-reward-datasets --data_split 2,4,4 --model_name_or_path facebook/opt-350m --num_padding_at_beginning 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --max_seq_len 512 --learning_rate 5e-5 --weight_decay 0.1 --num_train_epochs 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --num_warmup_steps 0 --seed 1234 --zero_stage 0 --deepspeed --output_dir ./output_fourDatasets
***** Running training *****
***** Evaluating reward, Epoch 0/1 *****
chosen_last_scores (higher is better) : 2.8115079402923584, acc (higher is better) : 0.4924241304397583
Beginning of Epoch 1/1, Total Micro Batches 1840
[2023-04-18 03:36:00,501] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,502] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 0
[2023-04-18 03:36:00,503] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,746] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,746] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,745] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,746] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,746] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,746] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,746] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,746] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-04-18 03:36:00,746] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1
[2023-04-18 03:36:00,746] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,746] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,988] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,989] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 2
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:00,989] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,227] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 3
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,228] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-18 03:36:01,228] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 4
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,469] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 5
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,721] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,722] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,960] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 6
[2023-04-18 03:36:01,961] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,961] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,961] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
[2023-04-18 03:36:01,961] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1024.0, reducing to 512.0
[2023-04-18 03:36:02,198] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,198] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,198] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,198] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,198] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,198] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,198] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,198] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,198] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,198] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 7
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,199] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:36:02,199] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 512.0, reducing to 256.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 256.0, reducing to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 8
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,439] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 128.0, reducing to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 9
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,681] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:02,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[5e-05, 5e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:02,683] [INFO] [timer.py:199:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=265.320034984211, CurrSamplesPerSec=265.4349789974132, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:05,686] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=10, lr=[4.999635612423198e-05, 4.999635612423198e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:05,695] [INFO] [timer.py:199:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=233.55939410769662, CurrSamplesPerSec=213.49183724272422, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:08,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=10, lr=[4.998542555915435e-05, 4.998542555915435e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:08,642] [INFO] [timer.py:199:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=227.66776915298374, CurrSamplesPerSec=217.4545430979453, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:11,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=10, lr=[4.996721149113682e-05, 4.996721149113682e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:11,582] [INFO] [timer.py:199:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=225.11184596131628, CurrSamplesPerSec=218.65992852947812, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:14,512] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=10, lr=[4.994171922976348e-05, 4.994171922976348e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:14,522] [INFO] [timer.py:199:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=223.64021397905867, CurrSamplesPerSec=218.39876918771108, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:17,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=10, lr=[4.9908956206285e-05, 4.9908956206285e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:17,477] [INFO] [timer.py:199:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=222.50605211780783, CurrSamplesPerSec=217.6126698050419, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:20,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=10, lr=[4.986893197145237e-05, 4.986893197145237e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:20,421] [INFO] [timer.py:199:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=221.82950715661497, CurrSamplesPerSec=216.39103240519432, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:23,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=10, lr=[4.982165819273275e-05, 4.982165819273275e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:23,354] [INFO] [timer.py:199:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=221.42476663458936, CurrSamplesPerSec=220.97345211419955, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:26,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=10, lr=[4.976714865090827e-05, 4.976714865090827e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:26,280] [INFO] [timer.py:199:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=221.18608845437842, CurrSamplesPerSec=219.40429956010507, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:29,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=10, lr=[4.9705419236058825e-05, 4.9705419236058825e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:29,229] [INFO] [timer.py:199:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=220.81515390033704, CurrSamplesPerSec=216.64933270488575, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:32,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=10, lr=[4.963648794292992e-05, 4.963648794292992e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:32,157] [INFO] [timer.py:199:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=220.65748563686077, CurrSamplesPerSec=220.54227501538415, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:32,419] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,421] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,420] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,421] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,421] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,421] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:32,423] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:36:32,423] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:36:35,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=10, lr=[4.956037486568706e-05, 4.956037486568706e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:35,094] [INFO] [timer.py:199:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=220.4744540789782, CurrSamplesPerSec=219.14636945940694, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:38,033] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=10, lr=[4.947710219205808e-05, 4.947710219205808e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:38,042] [INFO] [timer.py:199:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=220.24658400459697, CurrSamplesPerSec=214.50108794581607, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:40,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=10, lr=[4.938669419686516e-05, 4.938669419686516e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:40,982] [INFO] [timer.py:199:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=220.10165949901187, CurrSamplesPerSec=217.11281014763176, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 128.0, reducing to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 145
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:42,687] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:36:43,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=11, lr=[4.929924804067349e-05, 4.929924804067349e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:43,855] [INFO] [timer.py:199:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=220.320079789043, CurrSamplesPerSec=220.6193093653841, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:46,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=11, lr=[4.919535725504757e-05, 4.919535725504757e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:46,768] [INFO] [timer.py:199:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=220.31818819882218, CurrSamplesPerSec=220.48847352434333, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:49,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=11, lr=[4.908441327934164e-05, 4.908441327934164e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:49,745] [INFO] [timer.py:199:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=220.02737580499908, CurrSamplesPerSec=216.1091609647629, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:52,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=11, lr=[4.8966448454840854e-05, 4.8966448454840854e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:52,747] [INFO] [timer.py:199:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=219.67021770257563, CurrSamplesPerSec=215.22133520624925, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:55,722] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=11, lr=[4.884149716947845e-05, 4.884149716947845e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:55,730] [INFO] [timer.py:199:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=219.419188688239, CurrSamplesPerSec=215.64631606007416, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:36:58,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=11, lr=[4.8709595847811294e-05, 4.8709595847811294e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:36:58,643] [INFO] [timer.py:199:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=219.46665944666333, CurrSamplesPerSec=221.22695183922121, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:01,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=11, lr=[4.8570782940401785e-05, 4.8570782940401785e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:01,562] [INFO] [timer.py:199:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=219.4836312865479, CurrSamplesPerSec=216.87810781878042, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:04,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=11, lr=[4.8425098912609085e-05, 4.8425098912609085e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:04,477] [INFO] [timer.py:199:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=219.51293370647764, CurrSamplesPerSec=220.4520773057153, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:07,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=11, lr=[4.8272586232793085e-05, 4.8272586232793085e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:07,419] [INFO] [timer.py:199:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=219.44858308200799, CurrSamplesPerSec=219.03836255363038, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:10,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=11, lr=[4.8113289359934456e-05, 4.8113289359934456e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:10,363] [INFO] [timer.py:199:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=219.38869905137545, CurrSamplesPerSec=219.2817070999005, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:12,393] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,394] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,397] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,397] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:12,397] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:12,398] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:37:13,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=11, lr=[4.794725473067437e-05, 4.794725473067437e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:13,313] [INFO] [timer.py:199:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=219.31216463612918, CurrSamplesPerSec=213.66516786499164, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:16,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=11, lr=[4.777453074577784e-05, 4.777453074577784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:16,257] [INFO] [timer.py:199:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=219.25938356180865, CurrSamplesPerSec=218.80625814097934, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:19,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=11, lr=[4.759516775602428e-05, 4.759516775602428e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:19,182] [INFO] [timer.py:199:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=219.26356247333504, CurrSamplesPerSec=221.033496479082, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:22,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=11, lr=[4.740921804752989e-05, 4.740921804752989e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:22,123] [INFO] [timer.py:199:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=219.22444278910163, CurrSamplesPerSec=215.34547473233783, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:25,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=11, lr=[4.721673582650558e-05, 4.721673582650558e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:25,056] [INFO] [timer.py:199:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=219.20790670651408, CurrSamplesPerSec=220.0526909036954, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:27,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=11, lr=[4.701777720345546e-05, 4.701777720345546e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:27,993] [INFO] [timer.py:199:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=219.18577943358747, CurrSamplesPerSec=217.16339548854904, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:30,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=11, lr=[4.681240017681993e-05, 4.681240017681993e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:30,940] [INFO] [timer.py:199:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=219.13717817881815, CurrSamplesPerSec=216.56543542942705, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:33,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=11, lr=[4.660066461606867e-05, 4.660066461606867e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:33,918] [INFO] [timer.py:199:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=219.01961423882202, CurrSamplesPerSec=215.87262945419747, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:36,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=11, lr=[4.638263224424798e-05, 4.638263224424798e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:36,879] [INFO] [timer.py:199:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=218.95032322695113, CurrSamplesPerSec=220.3776925792442, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:39,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=11, lr=[4.615836661998799e-05, 4.615836661998799e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:39,811] [INFO] [timer.py:199:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=218.94708216062656, CurrSamplesPerSec=220.17975909741503, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,822] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:37:41,823] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:37:42,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=11, lr=[4.5927933118974595e-05, 4.5927933118974595e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:42,726] [INFO] [timer.py:199:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=218.9798057221148, CurrSamplesPerSec=220.57634475045873, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:45,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=11, lr=[4.569139891489183e-05, 4.569139891489183e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:45,649] [INFO] [timer.py:199:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=218.99475182710077, CurrSamplesPerSec=217.46564148346417, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:48,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=11, lr=[4.544883295984006e-05, 4.544883295984006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:48,592] [INFO] [timer.py:199:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=218.96971387840748, CurrSamplesPerSec=217.18641259320628, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:51,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=11, lr=[4.520030596423575e-05, 4.520030596423575e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:51,541] [INFO] [timer.py:199:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=218.93275165132903, CurrSamplesPerSec=216.3172707009398, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:54,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=11, lr=[4.494589037619867e-05, 4.494589037619867e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:54,493] [INFO] [timer.py:199:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=218.8924833295146, CurrSamplesPerSec=216.60597766445034, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:37:57,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=11, lr=[4.468566036043251e-05, 4.468566036043251e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:37:57,450] [INFO] [timer.py:199:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=218.84628950410894, CurrSamplesPerSec=218.59048605529674, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:00,364] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=11, lr=[4.4419691776605146e-05, 4.4419691776605146e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:00,374] [INFO] [timer.py:199:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=218.8611148975697, CurrSamplesPerSec=218.8708408618002, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:03,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=11, lr=[4.41480621572348e-05, 4.41480621572348e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:03,293] [INFO] [timer.py:199:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=218.88460348315985, CurrSamplesPerSec=219.06463919906705, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:06,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=11, lr=[4.387085068508852e-05, 4.387085068508852e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:06,230] [INFO] [timer.py:199:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=218.87527237966157, CurrSamplesPerSec=217.19906529351985, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:09,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=11, lr=[4.358813817009955e-05, 4.358813817009955e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:09,169] [INFO] [timer.py:199:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=218.86277469612367, CurrSamplesPerSec=217.1229955780379, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:11,190] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,192] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,192] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:11,191] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:38:11,192] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:38:12,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=11, lr=[4.330000702581053e-05, 4.330000702581053e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:12,107] [INFO] [timer.py:199:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=218.8508557931614, CurrSamplesPerSec=217.3390462310744, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:15,015] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=11, lr=[4.300654124534902e-05, 4.300654124534902e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:15,025] [INFO] [timer.py:199:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=218.87510562266604, CurrSamplesPerSec=220.14310364057445, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:17,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=11, lr=[4.270782637694273e-05, 4.270782637694273e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:17,949] [INFO] [timer.py:199:stop] epoch=0/micro_step=470/global_step=470, RunningAvgSamplesPerSec=218.88637924733757, CurrSamplesPerSec=219.62473675512132, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:20,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=11, lr=[4.2403949498981285e-05, 4.2403949498981285e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:20,893] [INFO] [timer.py:199:stop] epoch=0/micro_step=480/global_step=480, RunningAvgSamplesPerSec=218.8670040828307, CurrSamplesPerSec=217.1164978655188, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 512.0, reducing to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:21,727] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 482
[2023-04-18 03:38:21,728] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 256.0, reducing to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 485
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,561] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,803] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,804] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,804] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,804] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,804] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,804] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 128.0, reducing to 64.0
[2023-04-18 03:38:22,804] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,804] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 486
[2023-04-18 03:38:22,804] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:22,804] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
[2023-04-18 03:38:23,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=14, lr=[4.2188211665338126e-05, 4.2188211665338126e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:23,681] [INFO] [timer.py:199:stop] epoch=0/micro_step=490/global_step=490, RunningAvgSamplesPerSec=219.08700473431057, CurrSamplesPerSec=221.22695183922121, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:26,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=14, lr=[4.187576346253234e-05, 4.187576346253234e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:26,600] [INFO] [timer.py:199:stop] epoch=0/micro_step=500/global_step=500, RunningAvgSamplesPerSec=219.10175871073957, CurrSamplesPerSec=218.45937794502777, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:29,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=14, lr=[4.1558395804882695e-05, 4.1558395804882695e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:29,514] [INFO] [timer.py:199:stop] epoch=0/micro_step=510/global_step=510, RunningAvgSamplesPerSec=219.1241906891609, CurrSamplesPerSec=220.76536908755514, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:32,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=14, lr=[4.123620120825459e-05, 4.123620120825459e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:32,442] [INFO] [timer.py:199:stop] epoch=0/micro_step=520/global_step=520, RunningAvgSamplesPerSec=219.1237731404074, CurrSamplesPerSec=217.3276088619739, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:35,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=14, lr=[4.0909273595614694e-05, 4.0909273595614694e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:35,401] [INFO] [timer.py:199:stop] epoch=0/micro_step=530/global_step=530, RunningAvgSamplesPerSec=219.08096439007844, CurrSamplesPerSec=216.8374636598848, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:38,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=14, lr=[4.057770826965143e-05, 4.057770826965143e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:38,324] [INFO] [timer.py:199:stop] epoch=0/micro_step=540/global_step=540, RunningAvgSamplesPerSec=219.08926594426876, CurrSamplesPerSec=218.83676069806555, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:41,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=14, lr=[4.0241601884993366e-05, 4.0241601884993366e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:41,266] [INFO] [timer.py:199:stop] epoch=0/micro_step=550/global_step=550, RunningAvgSamplesPerSec=219.07058718904423, CurrSamplesPerSec=214.91617134658742, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 64.0, reducing to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,549] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,548] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 557
[2023-04-18 03:38:43,549] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,549] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,549] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 558
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,787] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,788] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
[2023-04-18 03:38:43,788] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32.0, reducing to 16.0
[2023-04-18 03:38:44,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=16, lr=[3.996951301273557e-05, 3.996951301273557e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:44,084] [INFO] [timer.py:199:stop] epoch=0/micro_step=560/global_step=560, RunningAvgSamplesPerSec=219.2193344837397, CurrSamplesPerSec=217.6075539731642, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16.0, reducing to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 565
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:45,785] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
[2023-04-18 03:38:46,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=17, lr=[3.9660077271631113e-05, 3.9660077271631113e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:46,957] [INFO] [timer.py:199:stop] epoch=0/micro_step=570/global_step=570, RunningAvgSamplesPerSec=219.29114140173343, CurrSamplesPerSec=220.04619703993345, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:49,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=17, lr=[3.931220308231662e-05, 3.931220308231662e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:49,880] [INFO] [timer.py:199:stop] epoch=0/micro_step=580/global_step=580, RunningAvgSamplesPerSec=219.2953114903082, CurrSamplesPerSec=220.74067958426633, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:52,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=17, lr=[3.896015674180224e-05, 3.896015674180224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:52,796] [INFO] [timer.py:199:stop] epoch=0/micro_step=590/global_step=590, RunningAvgSamplesPerSec=219.3079739104511, CurrSamplesPerSec=222.318579204198, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:55,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=17, lr=[3.8604040875138315e-05, 3.8604040875138315e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:55,716] [INFO] [timer.py:199:stop] epoch=0/micro_step=600/global_step=600, RunningAvgSamplesPerSec=219.31456684425606, CurrSamplesPerSec=220.3527279413696, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:38:58,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=17, lr=[3.8243959293683016e-05, 3.8243959293683016e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:38:58,643] [INFO] [timer.py:199:stop] epoch=0/micro_step=610/global_step=610, RunningAvgSamplesPerSec=219.31388985936573, CurrSamplesPerSec=218.47911415328144, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:01,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=17, lr=[3.788001696484028e-05, 3.788001696484028e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:01,559] [INFO] [timer.py:199:stop] epoch=0/micro_step=620/global_step=620, RunningAvgSamplesPerSec=219.3251462727111, CurrSamplesPerSec=219.21365935561005, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:04,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=17, lr=[3.751231998146076e-05, 3.751231998146076e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:04,477] [INFO] [timer.py:199:stop] epoch=0/micro_step=630/global_step=630, RunningAvgSamplesPerSec=219.33432077676605, CurrSamplesPerSec=220.1095285653608, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:07,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=17, lr=[3.714097553091465e-05, 3.714097553091465e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:07,402] [INFO] [timer.py:199:stop] epoch=0/micro_step=640/global_step=640, RunningAvgSamplesPerSec=219.33550379682038, CurrSamplesPerSec=220.08624864001547, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:10,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=17, lr=[3.6766091863845564e-05, 3.6766091863845564e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:10,332] [INFO] [timer.py:199:stop] epoch=0/micro_step=650/global_step=650, RunningAvgSamplesPerSec=219.32889220493504, CurrSamplesPerSec=215.10319903713165, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:13,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=17, lr=[3.6387778262614316e-05, 3.6387778262614316e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:13,300] [INFO] [timer.py:199:stop] epoch=0/micro_step=660/global_step=660, RunningAvgSamplesPerSec=219.2816918549144, CurrSamplesPerSec=215.42739193939298, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,342] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,342] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,341] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,342] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:15,343] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:15,344] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8.0 to 16.0
[2023-04-18 03:39:16,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=17, lr=[3.600614500944205e-05, 3.600614500944205e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:16,247] [INFO] [timer.py:199:stop] epoch=0/micro_step=670/global_step=670, RunningAvgSamplesPerSec=219.2586504715837, CurrSamplesPerSec=218.93403322072714, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:19,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=17, lr=[3.562130335426184e-05, 3.562130335426184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:19,181] [INFO] [timer.py:199:stop] epoch=0/micro_step=680/global_step=680, RunningAvgSamplesPerSec=219.24968712306404, CurrSamplesPerSec=219.3980232266375, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:22,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=17, lr=[3.5233365482288225e-05, 3.5233365482288225e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:22,121] [INFO] [timer.py:199:stop] epoch=0/micro_step=690/global_step=690, RunningAvgSamplesPerSec=219.23533015388068, CurrSamplesPerSec=219.24105244673856, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:25,040] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=17, lr=[3.4842444481314116e-05, 3.4842444481314116e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:25,050] [INFO] [timer.py:199:stop] epoch=0/micro_step=700/global_step=700, RunningAvgSamplesPerSec=219.2323164586272, CurrSamplesPerSec=216.52054213325223, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:27,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=17, lr=[3.444865430874453e-05, 3.444865430874453e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:27,977] [INFO] [timer.py:199:stop] epoch=0/micro_step=710/global_step=710, RunningAvgSamplesPerSec=219.23212802703992, CurrSamplesPerSec=221.20288811282165, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:30,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=17, lr=[3.405210975837685e-05, 3.405210975837685e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:30,910] [INFO] [timer.py:199:stop] epoch=0/micro_step=720/global_step=720, RunningAvgSamplesPerSec=219.22525321803266, CurrSamplesPerSec=220.43433624278077, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:33,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=17, lr=[3.365292642693732e-05, 3.365292642693732e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:33,831] [INFO] [timer.py:199:stop] epoch=0/micro_step=730/global_step=730, RunningAvgSamplesPerSec=219.23136998837057, CurrSamplesPerSec=220.43180203831426, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:36,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=17, lr=[3.3251220680383436e-05, 3.3251220680383436e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:36,755] [INFO] [timer.py:199:stop] epoch=0/micro_step=740/global_step=740, RunningAvgSamplesPerSec=219.23390280938014, CurrSamplesPerSec=219.93135508651073, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:39,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=17, lr=[3.284710961998203e-05, 3.284710961998203e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:39,705] [INFO] [timer.py:199:stop] epoch=0/micro_step=750/global_step=750, RunningAvgSamplesPerSec=219.21035715581846, CurrSamplesPerSec=216.05576142352604, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:42,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=17, lr=[3.244071104817317e-05, 3.244071104817317e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:42,675] [INFO] [timer.py:199:stop] epoch=0/micro_step=760/global_step=760, RunningAvgSamplesPerSec=219.16820189640498, CurrSamplesPerSec=215.81223278272572, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,723] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,723] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,723] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,723] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,723] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,723] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,723] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,723] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:44,725] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:39:44,725] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16.0 to 32.0
[2023-04-18 03:39:45,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=17, lr=[3.203214343422948e-05, 3.203214343422948e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:45,639] [INFO] [timer.py:199:stop] epoch=0/micro_step=770/global_step=770, RunningAvgSamplesPerSec=219.13204800694217, CurrSamplesPerSec=218.72816755428371, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:48,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=17, lr=[3.1621525879721206e-05, 3.1621525879721206e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:48,576] [INFO] [timer.py:199:stop] epoch=0/micro_step=780/global_step=780, RunningAvgSamplesPerSec=219.12295093958474, CurrSamplesPerSec=219.23782937480553, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:51,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=17, lr=[3.12089780837969e-05, 3.12089780837969e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:51,505] [INFO] [timer.py:199:stop] epoch=0/micro_step=790/global_step=790, RunningAvgSamplesPerSec=219.12234353762653, CurrSamplesPerSec=220.65612359056433, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:54,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=17, lr=[3.079462030828989e-05, 3.079462030828989e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:54,422] [INFO] [timer.py:199:stop] epoch=0/micro_step=800/global_step=800, RunningAvgSamplesPerSec=219.13262358523212, CurrSamplesPerSec=221.18447925103843, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:39:57,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=17, lr=[3.0378573342660782e-05, 3.0378573342660782e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:39:57,357] [INFO] [timer.py:199:stop] epoch=0/micro_step=810/global_step=810, RunningAvgSamplesPerSec=219.12705844488337, CurrSamplesPerSec=218.9522479608483, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:00,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=17, lr=[2.9960958468786083e-05, 2.9960958468786083e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:00,280] [INFO] [timer.py:199:stop] epoch=0/micro_step=820/global_step=820, RunningAvgSamplesPerSec=219.13208928029044, CurrSamplesPerSec=219.1562098168034, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:03,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=17, lr=[2.9541897425603337e-05, 2.9541897425603337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:03,213] [INFO] [timer.py:199:stop] epoch=0/micro_step=830/global_step=830, RunningAvgSamplesPerSec=219.12754408149513, CurrSamplesPerSec=216.3753342130145, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:06,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=17, lr=[2.912151237362299e-05, 2.912151237362299e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:06,165] [INFO] [timer.py:199:stop] epoch=0/micro_step=840/global_step=840, RunningAvgSamplesPerSec=219.10674347846955, CurrSamplesPerSec=216.18034339464614, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:09,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=17, lr=[2.8699925859317366e-05, 2.8699925859317366e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:09,090] [INFO] [timer.py:199:stop] epoch=0/micro_step=850/global_step=850, RunningAvgSamplesPerSec=219.1095320888808, CurrSamplesPerSec=219.04765697293928, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:12,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=17, lr=[2.827726077939718e-05, 2.827726077939718e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:12,012] [INFO] [timer.py:199:stop] epoch=0/micro_step=860/global_step=860, RunningAvgSamplesPerSec=219.11561001854562, CurrSamplesPerSec=218.65422902389972, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,031] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
[2023-04-18 03:40:14,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=17, lr=[2.785364034498582e-05, 2.785364034498582e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:14,932] [INFO] [timer.py:199:stop] epoch=0/micro_step=870/global_step=870, RunningAvgSamplesPerSec=219.12268985209582, CurrSamplesPerSec=221.35866582225037, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:17,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=17, lr=[2.742918804570216e-05, 2.742918804570216e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:17,863] [INFO] [timer.py:199:stop] epoch=0/micro_step=880/global_step=880, RunningAvgSamplesPerSec=219.120244124262, CurrSamplesPerSec=216.30472052512235, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:20,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=17, lr=[2.7004027613662043e-05, 2.7004027613662043e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:20,803] [INFO] [timer.py:199:stop] epoch=0/micro_step=890/global_step=890, RunningAvgSamplesPerSec=219.11010578229926, CurrSamplesPerSec=216.54604343417313, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:26,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=17, lr=[2.6578282987409136e-05, 2.6578282987409136e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:26,675] [INFO] [timer.py:199:stop] epoch=0/micro_step=900/global_step=900, RunningAvgSamplesPerSec=216.67885779656578, CurrSamplesPerSec=218.4060546789188, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:29,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=17, lr=[2.6152078275785596e-05, 2.6152078275785596e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:29,595] [INFO] [timer.py:199:stop] epoch=0/micro_step=910/global_step=910, RunningAvgSamplesPerSec=216.71176865058735, CurrSamplesPerSec=215.8580478523996, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:32,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=17, lr=[2.5725537721753102e-05, 2.5725537721753102e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:32,528] [INFO] [timer.py:199:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=216.7336629064403, CurrSamplesPerSec=215.78447553774203, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:35,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=17, lr=[2.529878566617475e-05, 2.529878566617475e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:35,490] [INFO] [timer.py:199:stop] epoch=0/micro_step=930/global_step=930, RunningAvgSamplesPerSec=216.73266086337313, CurrSamplesPerSec=215.84103847316655, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:38,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=17, lr=[2.4871946511568504e-05, 2.4871946511568504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:38,437] [INFO] [timer.py:199:stop] epoch=0/micro_step=940/global_step=940, RunningAvgSamplesPerSec=216.74402305310278, CurrSamplesPerSec=217.51092758912395, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:41,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=17, lr=[2.444514468584253e-05, 2.444514468584253e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:41,371] [INFO] [timer.py:199:stop] epoch=0/micro_step=950/global_step=950, RunningAvgSamplesPerSec=216.76441470792236, CurrSamplesPerSec=222.16273505461882, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:44,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=17, lr=[2.4018504606023293e-05, 2.4018504606023293e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:44,303] [INFO] [timer.py:199:stop] epoch=0/micro_step=960/global_step=960, RunningAvgSamplesPerSec=216.78565702628165, CurrSamplesPerSec=217.97827975814428, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:46,327] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,327] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,327] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,328] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:46,330] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:40:46,330] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
[2023-04-18 03:40:47,230] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=17, lr=[2.3592150641986648e-05, 2.3592150641986648e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:47,240] [INFO] [timer.py:199:stop] epoch=0/micro_step=970/global_step=970, RunningAvgSamplesPerSec=216.80311380259832, CurrSamplesPerSec=218.30925064918904, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:50,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=17, lr=[2.316620708020285e-05, 2.316620708020285e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:50,181] [INFO] [timer.py:199:stop] epoch=0/micro_step=980/global_step=980, RunningAvgSamplesPerSec=216.81646094209336, CurrSamplesPerSec=218.44924452097658, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:53,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=17, lr=[2.2740798087505783e-05, 2.2740798087505783e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:53,109] [INFO] [timer.py:199:stop] epoch=0/micro_step=990/global_step=990, RunningAvgSamplesPerSec=216.83947568519696, CurrSamplesPerSec=214.93458412368477, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:56,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=17, lr=[2.2316047674897034e-05, 2.2316047674897034e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:56,057] [INFO] [timer.py:199:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=216.84785356037364, CurrSamplesPerSec=219.07894206355877, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:40:58,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=17, lr=[2.1892079661395495e-05, 2.1892079661395495e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:40:58,993] [INFO] [timer.py:199:stop] epoch=0/micro_step=1010/global_step=1010, RunningAvgSamplesPerSec=216.86448472481848, CurrSamplesPerSec=221.3320184891715, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:01,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=17, lr=[2.1469017637942804e-05, 2.1469017637942804e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:01,937] [INFO] [timer.py:199:stop] epoch=0/micro_step=1020/global_step=1020, RunningAvgSamplesPerSec=216.87509326467367, CurrSamplesPerSec=217.03802919277206, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:04,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=17, lr=[2.1046984931375433e-05, 2.1046984931375433e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:04,874] [INFO] [timer.py:199:stop] epoch=0/micro_step=1030/global_step=1030, RunningAvgSamplesPerSec=216.8904083994008, CurrSamplesPerSec=219.79305499832148, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:07,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=17, lr=[2.0626104568473596e-05, 2.0626104568473596e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:07,795] [INFO] [timer.py:199:stop] epoch=0/micro_step=1040/global_step=1040, RunningAvgSamplesPerSec=216.91672794109778, CurrSamplesPerSec=219.5940141539262, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:10,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=17, lr=[2.0206499240097755e-05, 2.0206499240097755e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:10,741] [INFO] [timer.py:199:stop] epoch=0/micro_step=1050/global_step=1050, RunningAvgSamplesPerSec=216.925013568977, CurrSamplesPerSec=217.987838469934, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:13,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=17, lr=[1.9788291265422945e-05, 1.9788291265422945e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:13,664] [INFO] [timer.py:199:stop] epoch=0/micro_step=1060/global_step=1060, RunningAvgSamplesPerSec=216.94935165227335, CurrSamplesPerSec=218.1923122566591, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,679] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,678] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,679] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,679] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,679] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,679] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,679] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,679] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:15,680] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:15,680] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 128.0 to 256.0
[2023-04-18 03:41:16,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=17, lr=[1.937160255628156e-05, 1.937160255628156e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:16,586] [INFO] [timer.py:199:stop] epoch=0/micro_step=1070/global_step=1070, RunningAvgSamplesPerSec=216.97340233687692, CurrSamplesPerSec=219.05963363769678, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:19,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=17, lr=[1.8956554581624824e-05, 1.8956554581624824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:19,509] [INFO] [timer.py:199:stop] epoch=0/micro_step=1080/global_step=1080, RunningAvgSamplesPerSec=216.9969208728228, CurrSamplesPerSec=220.15501983921948, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:22,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=17, lr=[1.8543268332113316e-05, 1.8543268332113316e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:22,434] [INFO] [timer.py:199:stop] epoch=0/micro_step=1090/global_step=1090, RunningAvgSamplesPerSec=217.01894481846637, CurrSamplesPerSec=217.5128663258551, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:25,366] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=17, lr=[1.8131864284847043e-05, 1.8131864284847043e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:25,376] [INFO] [timer.py:199:stop] epoch=0/micro_step=1100/global_step=1100, RunningAvgSamplesPerSec=217.0289353713407, CurrSamplesPerSec=219.151200069884, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:28,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=17, lr=[1.7722462368245068e-05, 1.7722462368245068e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:28,308] [INFO] [timer.py:199:stop] epoch=0/micro_step=1110/global_step=1110, RunningAvgSamplesPerSec=217.0449729781252, CurrSamplesPerSec=216.28746641098053, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:31,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=17, lr=[1.7315181927085277e-05, 1.7315181927085277e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:31,249] [INFO] [timer.py:199:stop] epoch=0/micro_step=1120/global_step=1120, RunningAvgSamplesPerSec=217.0554655935377, CurrSamplesPerSec=217.53084534902848, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:34,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=17, lr=[1.691014168771409e-05, 1.691014168771409e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:34,180] [INFO] [timer.py:199:stop] epoch=0/micro_step=1130/global_step=1130, RunningAvgSamplesPerSec=217.07169920758096, CurrSamplesPerSec=220.80677270682065, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:37,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=17, lr=[1.6507459723436585e-05, 1.6507459723436585e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:37,106] [INFO] [timer.py:199:stop] epoch=0/micro_step=1140/global_step=1140, RunningAvgSamplesPerSec=217.09069511023836, CurrSamplesPerSec=220.00561907337743, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:40,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=17, lr=[1.6107253420096892e-05, 1.6107253420096892e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:40,028] [INFO] [timer.py:199:stop] epoch=0/micro_step=1150/global_step=1150, RunningAvgSamplesPerSec=217.11223821687588, CurrSamplesPerSec=220.45298253855364, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:42,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=17, lr=[1.5709639441859087e-05, 1.5709639441859087e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:42,949] [INFO] [timer.py:199:stop] epoch=0/micro_step=1160/global_step=1160, RunningAvgSamplesPerSec=217.1341331002338, CurrSamplesPerSec=219.69339959275337, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,983] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,984] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,984] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,984] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,984] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,984] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,984] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,984] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,984] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:44,986] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:41:44,987] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 256.0 to 512.0
[2023-04-18 03:41:45,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=17, lr=[1.5314733697198407e-05, 1.5314733697198407e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:45,900] [INFO] [timer.py:199:stop] epoch=0/micro_step=1170/global_step=1170, RunningAvgSamplesPerSec=217.13627939001756, CurrSamplesPerSec=221.81888616013143, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:48,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=17, lr=[1.4922651305112744e-05, 1.4922651305112744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:48,819] [INFO] [timer.py:199:stop] epoch=0/micro_step=1180/global_step=1180, RunningAvgSamplesPerSec=217.15881063831398, CurrSamplesPerSec=219.60227786817586, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:51,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=17, lr=[1.4533506561564306e-05, 1.4533506561564306e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:51,777] [INFO] [timer.py:199:stop] epoch=0/micro_step=1190/global_step=1190, RunningAvgSamplesPerSec=217.1567445267269, CurrSamplesPerSec=220.43524132992596, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:54,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=17, lr=[1.4147412906161172e-05, 1.4147412906161172e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:54,698] [INFO] [timer.py:199:stop] epoch=0/micro_step=1200/global_step=1200, RunningAvgSamplesPerSec=217.17757919615823, CurrSamplesPerSec=219.42582114416595, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:41:57,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=17, lr=[1.3764482889088581e-05, 1.3764482889088581e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:41:57,641] [INFO] [timer.py:199:stop] epoch=0/micro_step=1210/global_step=1210, RunningAvgSamplesPerSec=217.18479910748903, CurrSamplesPerSec=215.16975324475453, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:00,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=17, lr=[1.338482813829931e-05, 1.338482813829931e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:00,574] [INFO] [timer.py:199:stop] epoch=0/micro_step=1220/global_step=1220, RunningAvgSamplesPerSec=217.19835756559223, CurrSamplesPerSec=216.71772208412284, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:03,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=17, lr=[1.3008559326973116e-05, 1.3008559326973116e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:03,519] [INFO] [timer.py:199:stop] epoch=0/micro_step=1230/global_step=1230, RunningAvgSamplesPerSec=217.203575721669, CurrSamplesPerSec=217.4679317675118, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:06,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=17, lr=[1.2635786141254291e-05, 1.2635786141254291e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:06,470] [INFO] [timer.py:199:stop] epoch=0/micro_step=1240/global_step=1240, RunningAvgSamplesPerSec=217.20517683613562, CurrSamplesPerSec=217.2358016229029, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:09,405] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=17, lr=[1.2266617248277102e-05, 1.2266617248277102e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:09,415] [INFO] [timer.py:199:stop] epoch=0/micro_step=1250/global_step=1250, RunningAvgSamplesPerSec=217.21076953706302, CurrSamplesPerSec=220.21136842796085, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:12,329] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=17, lr=[1.1901160264488243e-05, 1.1901160264488243e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:12,339] [INFO] [timer.py:199:stop] epoch=0/micro_step=1260/global_step=1260, RunningAvgSamplesPerSec=217.22827776893337, CurrSamplesPerSec=218.91546444887726, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,371] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,372] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:14,374] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:14,374] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 512.0 to 1024.0
[2023-04-18 03:42:15,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=17, lr=[1.153952172427549e-05, 1.153952172427549e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:15,280] [INFO] [timer.py:199:stop] epoch=0/micro_step=1270/global_step=1270, RunningAvgSamplesPerSec=217.23560765896335, CurrSamplesPerSec=220.08678997769906, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:18,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=17, lr=[1.118180704891194e-05, 1.118180704891194e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:18,211] [INFO] [timer.py:199:stop] epoch=0/micro_step=1280/global_step=1280, RunningAvgSamplesPerSec=217.24872271148755, CurrSamplesPerSec=220.0830006698363, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:21,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=17, lr=[1.082812051582458e-05, 1.082812051582458e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:21,178] [INFO] [timer.py:199:stop] epoch=0/micro_step=1290/global_step=1290, RunningAvgSamplesPerSec=217.24095677346082, CurrSamplesPerSec=216.26359404531914, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:24,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=17, lr=[1.0478565228196391e-05, 1.0478565228196391e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:24,112] [INFO] [timer.py:199:stop] epoch=0/micro_step=1300/global_step=1300, RunningAvgSamplesPerSec=217.2519142662411, CurrSamplesPerSec=212.3059966149417, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:27,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=17, lr=[1.0133243084910764e-05, 1.0133243084910764e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:27,040] [INFO] [timer.py:199:stop] epoch=0/micro_step=1310/global_step=1310, RunningAvgSamplesPerSec=217.26660770167967, CurrSamplesPerSec=219.68387050838967, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:29,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=17, lr=[9.792254750846891e-06, 9.792254750846891e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:29,972] [INFO] [timer.py:199:stop] epoch=0/micro_step=1320/global_step=1320, RunningAvgSamplesPerSec=217.27841480091433, CurrSamplesPerSec=216.23084346278083, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:32,943] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=17, lr=[9.455699627535e-06, 9.455699627535e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:32,951] [INFO] [timer.py:199:stop] epoch=0/micro_step=1330/global_step=1330, RunningAvgSamplesPerSec=217.26411056701554, CurrSamplesPerSec=217.46564148346417, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:35,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=17, lr=[9.123675824179758e-06, 9.123675824179758e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:35,905] [INFO] [timer.py:199:stop] epoch=0/micro_step=1340/global_step=1340, RunningAvgSamplesPerSec=217.26390829428763, CurrSamplesPerSec=217.6611939620781, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:38,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=17, lr=[8.796280129060475e-06, 8.796280129060475e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:38,864] [INFO] [timer.py:199:stop] epoch=0/micro_step=1350/global_step=1350, RunningAvgSamplesPerSec=217.26124941313205, CurrSamplesPerSec=211.574576887033, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:41,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=17, lr=[8.473607981316364e-06, 8.473607981316364e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:41,805] [INFO] [timer.py:199:stop] epoch=0/micro_step=1360/global_step=1360, RunningAvgSamplesPerSec=217.26811742461774, CurrSamplesPerSec=217.4402754095705, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,833] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,834] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,834] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,835] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,836] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,836] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:43,836] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:42:43,836] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:42:44,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=17, lr=[8.155753443125036e-06, 8.155753443125036e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:44,749] [INFO] [timer.py:199:stop] epoch=0/micro_step=1370/global_step=1370, RunningAvgSamplesPerSec=217.27282407195733, CurrSamplesPerSec=218.48960602183305, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:47,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=17, lr=[7.842809172282436e-06, 7.842809172282436e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:47,690] [INFO] [timer.py:199:stop] epoch=0/micro_step=1380/global_step=1380, RunningAvgSamplesPerSec=217.27975786360386, CurrSamplesPerSec=221.48889195096532, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:50,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=17, lr=[7.534866395192203e-06, 7.534866395192203e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:50,632] [INFO] [timer.py:199:stop] epoch=0/micro_step=1390/global_step=1390, RunningAvgSamplesPerSec=217.2853138593823, CurrSamplesPerSec=218.33322163263048, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:53,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=17, lr=[7.2320148802721925e-06, 7.2320148802721925e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:53,560] [INFO] [timer.py:199:stop] epoch=0/micro_step=1400/global_step=1400, RunningAvgSamplesPerSec=217.29870563856866, CurrSamplesPerSec=217.69702749805566, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:56,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=17, lr=[6.934342911786143e-06, 6.934342911786143e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:56,511] [INFO] [timer.py:199:stop] epoch=0/micro_step=1410/global_step=1410, RunningAvgSamplesPerSec=217.29962425224832, CurrSamplesPerSec=219.6407302826069, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:42:59,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=17, lr=[6.641937264107867e-06, 6.641937264107867e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:42:59,448] [INFO] [timer.py:199:stop] epoch=0/micro_step=1420/global_step=1420, RunningAvgSamplesPerSec=217.30762557862326, CurrSamplesPerSec=219.63947228031589, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:02,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=17, lr=[6.35488317642568e-06, 6.35488317642568e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:02,396] [INFO] [timer.py:199:stop] epoch=0/micro_step=1430/global_step=1430, RunningAvgSamplesPerSec=217.3096154943934, CurrSamplesPerSec=216.95155119267181, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:05,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=17, lr=[6.073264327894332e-06, 6.073264327894332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:05,343] [INFO] [timer.py:199:stop] epoch=0/micro_step=1440/global_step=1440, RunningAvgSamplesPerSec=217.31228257026677, CurrSamplesPerSec=215.56838694393474, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:08,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=17, lr=[5.79716281324165e-06, 5.79716281324165e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:08,293] [INFO] [timer.py:199:stop] epoch=0/micro_step=1450/global_step=1450, RunningAvgSamplesPerSec=217.31479805624969, CurrSamplesPerSec=219.14744290999815, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:11,208] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=17, lr=[5.526659118837144e-06, 5.526659118837144e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:11,217] [INFO] [timer.py:199:stop] epoch=0/micro_step=1460/global_step=1460, RunningAvgSamplesPerSec=217.32909250211313, CurrSamplesPerSec=219.80673318954436, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,252] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,252] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,252] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,252] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,252] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,252] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,252] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,252] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,252] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:13,255] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:43:13,255] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0
[2023-04-18 03:43:14,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=17, lr=[5.261832099229388e-06, 5.261832099229388e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:14,166] [INFO] [timer.py:199:stop] epoch=0/micro_step=1470/global_step=1470, RunningAvgSamplesPerSec=217.33108525374053, CurrSamplesPerSec=217.54917996250933, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:17,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=17, lr=[5.0027589541591284e-06, 5.0027589541591284e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:17,121] [INFO] [timer.py:199:stop] epoch=0/micro_step=1480/global_step=1480, RunningAvgSamplesPerSec=217.32970469031258, CurrSamplesPerSec=216.65457844297265, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:20,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=17, lr=[4.749515206054822e-06, 4.749515206054822e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:20,070] [INFO] [timer.py:199:stop] epoch=0/micro_step=1490/global_step=1490, RunningAvgSamplesPerSec=217.33090348603076, CurrSamplesPerSec=218.09481158902196, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:23,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=17, lr=[4.502174678017018e-06, 4.502174678017018e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:23,020] [INFO] [timer.py:199:stop] epoch=0/micro_step=1500/global_step=1500, RunningAvgSamplesPerSec=217.331954380102, CurrSamplesPerSec=217.49577138412425, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:25,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=17, lr=[4.26080947229826e-06, 4.26080947229826e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:25,977] [INFO] [timer.py:199:stop] epoch=0/micro_step=1510/global_step=1510, RunningAvgSamplesPerSec=217.32946125601498, CurrSamplesPerSec=216.20228190535715, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:28,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=17, lr=[4.025489949284492e-06, 4.025489949284492e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:28,927] [INFO] [timer.py:199:stop] epoch=0/micro_step=1520/global_step=1520, RunningAvgSamplesPerSec=217.33111018956131, CurrSamplesPerSec=219.93477877926102, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:31,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=17, lr=[3.7962847069843126e-06, 3.7962847069843126e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:31,863] [INFO] [timer.py:199:stop] epoch=0/micro_step=1530/global_step=1530, RunningAvgSamplesPerSec=217.33859122373565, CurrSamplesPerSec=217.16058457344178, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:34,798] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=17, lr=[3.5732605610320074e-06, 3.5732605610320074e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:34,808] [INFO] [timer.py:199:stop] epoch=0/micro_step=1540/global_step=1540, RunningAvgSamplesPerSec=217.34334004578542, CurrSamplesPerSec=220.29829570401205, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,342] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,343] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,343] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,342] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,343] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,343] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,343] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:35,345] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1541
[2023-04-18 03:43:35,345] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-18 03:43:37,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=18, lr=[3.3778774384813557e-06, 3.3778774384813557e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:37,676] [INFO] [timer.py:199:stop] epoch=0/micro_step=1550/global_step=1550, RunningAvgSamplesPerSec=217.38321677804396, CurrSamplesPerSec=220.2500344608033, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:40,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=18, lr=[3.166774984049342e-06, 3.166774984049342e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:40,595] [INFO] [timer.py:199:stop] epoch=0/micro_step=1560/global_step=1560, RunningAvgSamplesPerSec=217.39874179250074, CurrSamplesPerSec=221.5183190597773, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:43,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=18, lr=[2.962037134383211e-06, 2.962037134383211e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:43,534] [INFO] [timer.py:199:stop] epoch=0/micro_step=1570/global_step=1570, RunningAvgSamplesPerSec=217.404440774079, CurrSamplesPerSec=220.7784422371344, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:46,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=18, lr=[2.763723572626087e-06, 2.763723572626087e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:46,452] [INFO] [timer.py:199:stop] epoch=0/micro_step=1580/global_step=1580, RunningAvgSamplesPerSec=217.4199392258332, CurrSamplesPerSec=219.78603658741278, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:49,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=18, lr=[2.5718921091765517e-06, 2.5718921091765517e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:49,383] [INFO] [timer.py:199:stop] epoch=0/micro_step=1590/global_step=1590, RunningAvgSamplesPerSec=217.42940829622682, CurrSamplesPerSec=220.160978422317, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:52,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=18, lr=[2.386598664836298e-06, 2.386598664836298e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:52,332] [INFO] [timer.py:199:stop] epoch=0/micro_step=1600/global_step=1600, RunningAvgSamplesPerSec=217.43048317864347, CurrSamplesPerSec=217.32303425138298, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:55,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=18, lr=[2.2078972545086645e-06, 2.2078972545086645e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:55,260] [INFO] [timer.py:199:stop] epoch=0/micro_step=1610/global_step=1610, RunningAvgSamplesPerSec=217.44130438893876, CurrSamplesPerSec=220.9345316872428, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,669] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:56,668] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1614
[2023-04-18 03:43:56,669] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:43:58,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=19, lr=[2.0527452693256287e-06, 2.0527452693256287e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:43:58,141] [INFO] [timer.py:199:stop] epoch=0/micro_step=1620/global_step=1620, RunningAvgSamplesPerSec=217.47328778409326, CurrSamplesPerSec=216.91350867822254, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:01,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=19, lr=[1.8867106400655533e-06, 1.8867106400655533e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:01,070] [INFO] [timer.py:199:stop] epoch=0/micro_step=1630/global_step=1630, RunningAvgSamplesPerSec=217.482976618043, CurrSamplesPerSec=218.0386553243059, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:04,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=19, lr=[1.7274137672069145e-06, 1.7274137672069145e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:04,011] [INFO] [timer.py:199:stop] epoch=0/micro_step=1640/global_step=1640, RunningAvgSamplesPerSec=217.48714198012703, CurrSamplesPerSec=218.5235781358754, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:06,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=19, lr=[1.5749010873909175e-06, 1.5749010873909175e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:06,966] [INFO] [timer.py:199:stop] epoch=0/micro_step=1650/global_step=1650, RunningAvgSamplesPerSec=217.48503430813517, CurrSamplesPerSec=214.09716685728688, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:09,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=19, lr=[1.4292170595982146e-06, 1.4292170595982146e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:09,904] [INFO] [timer.py:199:stop] epoch=0/micro_step=1660/global_step=1660, RunningAvgSamplesPerSec=217.49014632793464, CurrSamplesPerSec=218.5379883483511, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:12,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=19, lr=[1.2904041521887122e-06, 1.2904041521887122e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:12,839] [INFO] [timer.py:199:stop] epoch=0/micro_step=1670/global_step=1670, RunningAvgSamplesPerSec=217.49696279530096, CurrSamplesPerSec=218.54919760539494, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:15,771] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=19, lr=[1.15850283052156e-06, 1.15850283052156e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:15,781] [INFO] [timer.py:199:stop] epoch=0/micro_step=1680/global_step=1680, RunningAvgSamplesPerSec=217.50026662517845, CurrSamplesPerSec=218.11536811451006, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:18,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=19, lr=[1.0335515451591503e-06, 1.0335515451591503e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:18,722] [INFO] [timer.py:199:stop] epoch=0/micro_step=1690/global_step=1690, RunningAvgSamplesPerSec=217.50400749550178, CurrSamplesPerSec=218.01705083983143, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:21,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=19, lr=[9.155867206583624e-07, 9.155867206583624e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:21,665] [INFO] [timer.py:199:stop] epoch=0/micro_step=1700/global_step=1700, RunningAvgSamplesPerSec=217.50699718100392, CurrSamplesPerSec=217.72863434694094, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:24,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=19, lr=[8.046427449524274e-07, 8.046427449524274e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:24,613] [INFO] [timer.py:199:stop] epoch=0/micro_step=1710/global_step=1710, RunningAvgSamplesPerSec=217.50792668103418, CurrSamplesPerSec=217.65307568688692, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:26,349] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,349] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,349] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,349] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,350] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:26,353] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:26,353] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:27,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=19, lr=[7.007519593265204e-07, 7.007519593265204e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:27,564] [INFO] [timer.py:199:stop] epoch=0/micro_step=1720/global_step=1720, RunningAvgSamplesPerSec=217.50750395350983, CurrSamplesPerSec=217.20802851490276, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:28,979] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,979] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,979] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,979] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,979] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,979] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,979] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,979] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,979] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:28,980] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:362:_update_scale]
Grad overflow on iteration 1724
[2023-04-18 03:44:28,980] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
[2023-04-18 03:44:30,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=20, lr=[6.133058031348476e-07, 6.133058031348476e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:30,454] [INFO] [timer.py:199:stop] epoch=0/micro_step=1730/global_step=1730, RunningAvgSamplesPerSec=217.53283922313705, CurrSamplesPerSec=215.76574343343734, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:33,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=20, lr=[5.228978079419272e-07, 5.228978079419272e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:33,412] [INFO] [timer.py:199:stop] epoch=0/micro_step=1740/global_step=1740, RunningAvgSamplesPerSec=217.52932811848308, CurrSamplesPerSec=214.24924974778756, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:36,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=20, lr=[4.396251343129376e-07, 4.396251343129376e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:36,365] [INFO] [timer.py:199:stop] epoch=0/micro_step=1750/global_step=1750, RunningAvgSamplesPerSec=217.52763757327713, CurrSamplesPerSec=220.37515967700094, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:39,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=20, lr=[3.635120570700784e-07, 3.635120570700784e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:39,288] [INFO] [timer.py:199:stop] epoch=0/micro_step=1760/global_step=1760, RunningAvgSamplesPerSec=217.53900324463726, CurrSamplesPerSec=219.31072977362672, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:42,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=20, lr=[2.9458076394117684e-07, 2.9458076394117684e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:42,209] [INFO] [timer.py:199:stop] epoch=0/micro_step=1770/global_step=1770, RunningAvgSamplesPerSec=217.5508972956739, CurrSamplesPerSec=221.0833762977873, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:45,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=20, lr=[2.3285134909173112e-07, 2.3285134909173112e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:45,121] [INFO] [timer.py:199:stop] epoch=0/micro_step=1780/global_step=1780, RunningAvgSamplesPerSec=217.56657478330348, CurrSamplesPerSec=214.361142831589, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:48,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=20, lr=[1.7834180726725158e-07, 1.7834180726725158e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:48,036] [INFO] [timer.py:199:stop] epoch=0/micro_step=1790/global_step=1790, RunningAvgSamplesPerSec=217.5806861898962, CurrSamplesPerSec=220.5272369385267, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:50,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=20, lr=[1.31068028547629e-07, 1.31068028547629e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:50,983] [INFO] [timer.py:199:stop] epoch=0/micro_step=1800/global_step=1800, RunningAvgSamplesPerSec=217.58123717220835, CurrSamplesPerSec=217.04610165051298, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:53,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=20, lr=[9.104379371500105e-08, 9.104379371500105e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:53,931] [INFO] [timer.py:199:stop] epoch=0/micro_step=1810/global_step=1810, RunningAvgSamplesPerSec=217.58161057766557, CurrSamplesPerSec=211.20186311457996, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:56,876] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=20, lr=[5.8280770236518456e-08, 5.8280770236518456e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:56,885] [INFO] [timer.py:199:stop] epoch=0/micro_step=1820/global_step=1820, RunningAvgSamplesPerSec=217.57991981259295, CurrSamplesPerSec=220.0718139507429, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:44:58,608] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,608] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,615] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:58,616] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 100 iterations
[2023-04-18 03:44:58,617] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0
[2023-04-18 03:44:59,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=20, lr=[3.278850886317686e-08, 3.278850886317686e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:44:59,813] [INFO] [timer.py:199:stop] epoch=0/micro_step=1830/global_step=1830, RunningAvgSamplesPerSec=217.5884886843686, CurrSamplesPerSec=221.54592550470556, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
[2023-04-18 03:45:02,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=20, lr=[1.4574440845649407e-08, 1.4574440845649407e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-18 03:45:02,752] [INFO] [timer.py:199:stop] epoch=0/micro_step=1840/global_step=1840, RunningAvgSamplesPerSec=217.5927207451398, CurrSamplesPerSec=218.17688361995747, MemAllocated=4.34GB, MaxMemAllocated=12.81GB
Epoch 1/1 with loss 0.5754789632299672
***** Evaluating reward, Epoch 1/1 *****
chosen_last_scores (higher is better) : 2.243525981903076, acc (higher is better) : 0.6994950175285339
saving model ...
[2023-04-18 03:45:33,424] [INFO] [launch.py:460:main] Process 10393 exits successfully.
[2023-04-18 03:45:34,426] [INFO] [launch.py:460:main] Process 10388 exits successfully.
[2023-04-18 03:45:34,426] [INFO] [launch.py:460:main] Process 10396 exits successfully.
[2023-04-18 03:45:34,426] [INFO] [launch.py:460:main] Process 10389 exits successfully.
[2023-04-18 03:45:35,428] [INFO] [launch.py:460:main] Process 10401 exits successfully.
[2023-04-18 03:45:36,429] [INFO] [launch.py:460:main] Process 10397 exits successfully.
[2023-04-18 03:45:36,430] [INFO] [launch.py:460:main] Process 10390 exits successfully.
[2023-04-18 03:45:37,431] [INFO] [launch.py:460:main] Process 10399 exits successfully.
[2023-04-18 03:45:38,433] [INFO] [launch.py:460:main] Process 10387 exits successfully.
[2023-04-18 03:45:38,433] [INFO] [launch.py:460:main] Process 10391 exits successfully.
[2023-04-18 03:45:38,433] [INFO] [launch.py:460:main] Process 10398 exits successfully.
[2023-04-18 03:45:38,433] [INFO] [launch.py:460:main] Process 10392 exits successfully.
[2023-04-18 03:45:38,434] [INFO] [launch.py:460:main] Process 10402 exits successfully.
[2023-04-18 03:45:38,434] [INFO] [launch.py:460:main] Process 10400 exits successfully.
[2023-04-18 03:45:38,434] [INFO] [launch.py:460:main] Process 10395 exits successfully.
[2023-04-18 03:45:40,436] [INFO] [launch.py:460:main] Process 10394 exits successfully.
